#pragma once

#include "callbacks.h"
#include "chat.h"
#include "llama.h"
#include "model.h"
#include "tool.h"
#include <functional>
#include <memory>
#include <string>
#include <vector>

namespace agent_cpp {

class Agent
{
  private:
    std::vector<std::unique_ptr<Callback>> callbacks;
    std::string instructions;
    std::shared_ptr<Model> model;
    std::vector<std::unique_ptr<Tool>> tools;

    // Helper to ensure system message with instructions is at the start
    void ensure_system_message(std::vector<common_chat_msg>& messages);

  public:
    Agent(std::shared_ptr<Model> model,
          std::vector<std::unique_ptr<Tool>> tools,
          std::vector<std::unique_ptr<Callback>> callbacks = {},
          const std::string& instructions = "");

    // Run one turn of the agent loop
    // Assumes the latest user message is already in the messages vector
    // Executes tool calls as needed, and returns the final response
    // The callback is called for each token generated by the model
    std::string run_loop(std::vector<common_chat_msg>& messages,
                         const ResponseCallback& callback = nullptr);

    // Get the tool definitions for all registered tools
    // Useful for building prompts for caching
    [[nodiscard]] std::vector<common_chat_tool> get_tool_definitions() const;

    // Get the instructions string
    [[nodiscard]] const std::string& get_instructions() const
    {
        return instructions;
    }

    // Get the model (for cache operations)
    [[nodiscard]] Model* get_model() const { return model.get(); }

    // Load prompt cache from file, or create it if it doesn't exist
    // Returns true on success, false on failure
    bool load_or_create_cache(const std::string& cache_path);

  private:
    // Build the agent's prompt tokens (system message + tool definitions)
    std::vector<llama_token> build_prompt_tokens();
};

} // namespace agent_cpp
